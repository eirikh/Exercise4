%%%%%%%%%%%%%%%% Including packages %%%%%%%%%%%%%%%%%
\documentclass[english,12pt]{article}
\usepackage{amsmath,epsfig}
\usepackage[latin1]{inputenc}
\usepackage{calc, epsfig, rotating, amsmath}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{textcomp} 
\usepackage{epstopdf}
\usepackage{makeidx}
\usepackage{sidecap}
\usepackage{amssymb}
\usepackage{blkarray}
\usepackage{multirow}
\usepackage{float}
\usepackage{ae} %does also load the fontenc package with T1 option
\usepackage{babel}
\usepackage{ifthen,tikz,xkeyval}
\usepackage{todonotes}
\usepackage{fancyhdr, rotating}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage[intoc]{nomencl}
\usepackage{braket}
\usepackage{appendix}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage[section]{placeins}

%%%%%%%%%%%%%%%%% Title page %%%%%%%%%%%%%%%%%%%%%%%%%
\title{\textbf{TMA4280 - Exercise 4}}
\author{Rolf H. Myhre and Eirik Hjertenæs}
\date{17.02.14}


%%%%%%%%%%%%%%%%% Paragraph settings %%%%%%%%%%%%%%%%%
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5 ex minus 0.2ex}

%%%%%%%%%%%%%%%% Beginning document %%%%%%%%%%%%%%%%%%

\begin{document}
\bibliographystyle{jcp}

%%%%%%%%%%%%%%%% Redefining commands %%%%%%%%%%%%%%%%%
\renewcommand{\thesubsection}{\alph{subsection}}
\newcommand{\ra}{\ensuremath \rightarrow}

\makeatletter
\renewcommand{\subsection}{\@startsection{subsection}{1}{0mm}{0.5\baselineskip}{0.5 \baselineskip}{\normalfont\normalsize\textbf}}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\theenumi)}
\makeatother


\maketitle

%%%%%%%%%%%%%%%%%%%%%% Main part %%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Teaching goals %%%%%%%%%%%%%%%%%%%%%%%
\vspace{1.5cm}
\setcounter{tocdepth}{1}
\setcounter{secnumdepth}{0}
\setcounter{secnumdepth}{2}

\begin{center}
\section*{Summary}
In this exercise we have written a simple computer program that calculates the sum of the infinite series $S = \sum_{n=1}^{\infty}{\frac{1}{n^2}}$. This is referred to as the \emph{Basel problem} after the hometown of Leonhard Euler, the first who was able to show that the sum converges to $\frac{\pi^2}{6}$. Four versions of the computer program have been tested; a serial version, a parallelized code using OpenMP, a parallelized code using MPI and lastly a code using OpenMP and MPI in combination. We discuss the advantages and disadvantages of the versions of the program in terms of memory usage and efficiency. 
\end{center}

\newpage

\section{Introduction}
The computer programs we have written perform the simple task of generating the vector:	
	\begin{equation}
	v_i = \frac{1}{i^2}, i=1,\ldots,n
	\end{equation}
and then summing the elements
	\begin{equation}
	S_n = \sum_{i=1}^{n}{v_i}
	\end{equation}

We have implemented a computer code in Fortran which calculates $S_n$ for a given value of $n$ in serial, using MPI, using OpenMP and using OpenMP and MPI in combination. Below we display the sums $S_n$ for values og $n = 2^k$ where $k = 3,\ldots,14$ and provide timings for the various versions of the program.	

\section{Results}
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Table showing the sums of the vector elements in $v$ for different sizes and the resulting error.}
    \begin{tabular}{rrrrrrr}
    \toprule
    k     & n=2$^{k}$ & S$_n$ & S$_n$     & S$_n$     & S$_n$        & S-S$_n$ \\
          &           & (P=1) & MPI (P=2) & MPI (P=8) & OpenMP (P=8) &     \\
    \midrule
    3     & 8     & 1.584346533 & 1.584346533 & 1.584346533 & 1.584346533 & -6.06E-02 \\
    4     & 16    & 1.614167263 & 1.614167263 & 1.614167263 & 1.614167263 & -3.08E-02 \\
    5     & 32    & 1.629430501 & 1.629430501 & 1.629430501 & 1.629430501 & -1.55E-02 \\
    6     & 64    & 1.637152005 & 1.637152005 & 1.637152005 & 1.637152005 & -7.78E-03 \\
    7     & 128   & 1.641035436 & 1.641035436 & 1.641035436 & 1.641035436 & -3.90E-03 \\
    8     & 256   & 1.642982848 & 1.642982848 & 1.642982848 & 1.642982848 & -1.95E-03 \\
    9     & 512   & 1.643957981 & 1.643957981 & 1.643957981 & 1.643957981 & -9.76E-04 \\
    10    & 1024  & 1.644445905 & 1.644445905 & 1.644445905 & 1.644445905 & -4.88E-04 \\
    11    & 2048  & 1.644689956 & 1.644689956 & 1.644689956 & 1.644689956 & -2.44E-04 \\
    12    & 4096  & 1.644812004 & 1.644812004 & 1.644812004 & 1.644812004 & -1.22E-04 \\
    13    & 8192  & 1.644873034 & 1.644873034 & 1.644873034 & 1.644873034 & -6.10E-05 \\
    \bottomrule
    \end{tabular}%
  \label{tab:results}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Execution times for our codes on Stallo. The hybrid code was executed on two nodes with 16 processors each, while the others ran on one node.}
    \begin{tabular}{lr}
    \toprule
    Code  & Execution time (s) on Stallo \\
    \midrule
    Serial & 0.0000 \\
    MPI   & 1.2625 \\
    OpenMP & 0.0012 \\
    Hybrid & 1.1609$^*$ \\
    \bottomrule
    \end{tabular}%
  \label{tab:times}%
\end{table}%

\section{Discussion and Conclusions}
The serial and OpenMP/MPI hybrid codes are submitted along with the report online. The code compiles and works both on our personal computer and on Stallo. In the following section we discuss the results and the aspects mentioned in the exercise.
	
In MPI, the distribution of the vector \emph{v} from process 0 the others can be done in several ways, the two most obvious are MPI\_SEND and MPI\_SCATTER. MPI\_SCATTER is easier to use and, depending on the mpi implementation should be faster, as it allows for optimization~\cite{el-nashar}. Scatter distributes the elements of the vector \emph{v} evenly to all the processes. The code is also simplified as we remove a loop and the calls to MPI\_RECV. To collect the sum of the vector elements from each process we employ MPI\_REDUCE, as some of the elements must be summed.

As seen in Table~\ref{tab:results} the parallel and serial versions of the code yield exactly the same result. We have chosen an implementation where the highest value of $n$ is hard-coded in the program and the program then calculates every partial sum for all the values of $n$ up to the one specified. The partial sums of every value of $n$ is then reused which means all the sums are calculated the same way every time. We have chosen this implementation because this allows us to follow the convergence without re-running the program for every value of $n$ up to the one we are interested in. By storing all the partial sums, we use a bit more memory, however this is negligble. An advantage of this implementation is that the convergence rate is automatically provided.

The vector \emph{v} is generated using 1 floating point operation per element (the denominator is always and integer. Adding the elements to form the sum $S_n$ requires $n-1$ floating point operations. In the way the MPI-implementation is described in task 3, the program is not load balanced. The majority of the floating point operations are carried out on processor 0 while only the summation is divided between the processors. Because of the simple form of the vector one could easily envision an implementation where the vector elements are also calculated on each processor with minimal information sent to each node (each processor could determine which vector elements it owns based on the value of n, its rank and the size of MPI\_COMM\_WORLD). Since the highest value of $n$ is specified as a global variable in the program, this structure would eliminate the need to send information from processor 0 to the others.

In the serial code, the whole vector v must be stored, along with an array of the partial sums. The number of partial sums is neglible compared to the vector itself when we only consider sums for $n=2^k$.  Doing the calculation in parallel with MPI requires one new array to be allocated where the vector received from MPI\_SCATTER is stored. Every element of the vector $v$ then exists in two locations which means that the MPI code requires twice the amount of memory as the serial version when the number of elements of the vector becomes large.

In this program there are two loops, one where the elements of $v$ are calculated and one where they are added. The first loop is perfectly independent and can very efficiently be parallelized. The second loop is also suitable for parallelization since each processor can calculate its own partial sum. This is then clearly a case of \emph{data parallelism}. As mentioned above, the current implementation is not load balanced and these issues will have to be corrected before parallelization becomes truly efficient. Another aspect is the size of the problem. There is an overhead to be paid when parallelizing the code and for the low number of elements in v used here (<$2^{14}+1$), the benefit from parallelization is not greater than the cost of sending information between processors, as clearly seen in Table~\ref{tab:times}. This could be different when looking at larger vectors and a better implementation. The mathematics of the Basel problem constitutes very few FLOP per element of the vector. One could then argue that a distributed memory implementation would alleviate memory problems associated with very, very large values of $n$. However, with the rapid convergence of this series, calculating the sum with enough elements to exceed the storage of a normal computer would be ridiculous and not lead to any gains in accuracy. 

\begin{thebibliography}{9}

\bibitem{el-nashar} Alaa Ismail El-Nashar, \emph{International Journal of Distributed and Parallel Systems}, \textbf{2}, 2011.

\end{thebibliography}

\end{document}
